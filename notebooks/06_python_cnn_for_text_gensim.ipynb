{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 06. 파이썬을 이용한 CNN for Text (Using Gensim + Keras)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "* 싸이그래머 / 어바웃 파이썬\n",
    "* 김무성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 차례\n",
    "* Word2Vec (Gensim) + Deep Learning (Keras) 간단 예제 : 코사도 유사도  \n",
    "* 1D CNN for Text (Gensim + Keras)\n",
    "* 2D CNN for Text (Gensim + Keras)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Word2Vec (Gensim) + Deep Learning (Keras) 간단 예제 : 코사도 유사도 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    ['human', 'interface', 'computer'],\n",
    "    ['survey', 'user', 'computer', 'system', 'response', 'time'],\n",
    "    ['eps', 'user', 'interface', 'system'],\n",
    "    ['system', 'human', 'system', 'eps'],\n",
    "    ['user', 'response', 'time'],\n",
    "    ['trees'],\n",
    "    ['graph', 'trees'],\n",
    "    ['graph', 'minors', 'trees'],\n",
    "    ['graph', 'minors', 'survey']\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = word2vec.Word2Vec(sentences, size=100, min_count=1, hs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.engine import Input\n",
    "from keras.models import Model\n",
    "from keras.layers.merge import dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wv = model.wv\n",
    "embedding_layer = wv.get_keras_embedding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:3148: calling l2_normalize (from tensorflow.python.ops.nn_impl) with dim is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "dim is deprecated, use axis instead\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.5/site-packages/ipykernel/__main__.py:7: UserWarning: Update your `Model` call to the Keras 2 API: `Model(outputs=Tensor(\"do..., inputs=[<tf.Tenso...)`\n"
     ]
    }
   ],
   "source": [
    "input_a = Input(shape=(1,), dtype='int32', name='input_a')\n",
    "input_b = Input(shape=(1,), dtype='int32', name='input_b')\n",
    "embedding_a = embedding_layer(input_a)\n",
    "embedding_b = embedding_layer(input_b)\n",
    "similarity = dot([embedding_a, embedding_b], axes=2, normalize=True)\n",
    "\n",
    "keras_model = Model(input=[input_a, input_b], output=similarity)\n",
    "keras_model.compile(optimizer='sgd', loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 0.06383199]]]\n"
     ]
    }
   ],
   "source": [
    "word_a = 'graph'\n",
    "word_b = 'trees'\n",
    "# output is the cosine distance between the two words (as a similarity measure)\n",
    "output = keras_model.predict([np.asarray([model.wv.vocab[word_a].index]), np.asarray([model.wv.vocab[word_b].index])])\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "--------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1D CNN for Text (Gensim + Keras) \n",
    "* Data 준비\n",
    "* 딥러닝 모델에 입력가능한 포맷으로 변환\n",
    "* Keras에서 사용할 embedding layer 생성\n",
    "* CNN 모델 생성 및 학습\n",
    "* 학습된 모델로 새로운 문장"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 참고\n",
    "* [2] Using wrappers for Gensim models for working with Keras - https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/keras_wrapper.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data 준비 (영어 - 20newsgroups)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 필요한 라이브러리들"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import keras\n",
    "import numpy as np\n",
    "\n",
    "from gensim.models import word2vec\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.layers import Input, Dense, Flatten\n",
    "from keras.layers import Conv1D, MaxPooling1D\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 데이터 준비 결과는 아래 세 리스트에 담긴다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "texts = []  # list of text samples\n",
    "texts_w2v = []  # used to train the word embeddings\n",
    "labels = []  # list of label ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 20뉴스그룹 데이터의 일부분(카테고리 3가지 정도만)을 가져온다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#using 3 categories for training the classifier\n",
    "data = fetch_20newsgroups(subset='train', categories=['alt.atheism', 'comp.graphics', 'sci.space'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DESCR', 'data', 'description', 'filenames', 'target', 'target_names']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the 20 newsgroups by date dataset'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1657"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1657"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 1, 0, ..., 2, 2, 2])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alt.atheism', 'comp.graphics', 'sci.space']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.target_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 변환 과정을 한줄한줄 따져보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index: 0\n",
      "label_id: 2\n",
      "index: 1\n",
      "label_id: 1\n",
      "index: 2\n",
      "label_id: 0\n",
      "index: 3\n",
      "label_id: 1\n",
      "index: 4\n",
      "label_id: 2\n",
      "index: 5\n",
      "label_id: 2\n"
     ]
    }
   ],
   "source": [
    "for index in range(len(data)):\n",
    "    print(\"index:\", index)\n",
    "    label_id = data.target[index]\n",
    "    print(\"label_id:\",label_id)\n",
    "    #file_data = data.data[index]\n",
    "    #print(file_data)\n",
    "    #i = file_data.find('\\n\\n')  # skip header\n",
    "    #if i > 0:\n",
    "    #    file_data = file_data[i:]\n",
    "    #try:\n",
    "    #    curr_str = str(file_data)\n",
    "    #    sentence_list = curr_str.split('\\n')\n",
    "    #    for sentence in sentence_list:\n",
    "    #        sentence = (sentence.strip()).lower()\n",
    "    #        texts.append(sentence)\n",
    "    #        texts_w2v.append(sentence.split(' '))\n",
    "    #        labels.append(label_id)\n",
    "    #except:\n",
    "    #    None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index: 0\n",
      "label_id: 2\n",
      "file_data:\n",
      " From: degroff@netcom.com (21012d)\n",
      "Subject: Re: Venus Lander for Venus Conditions.\n",
      "Organization: Netcom Online Communications Services (408-241-9760 login: guest)\n",
      "Lines: 8\n",
      "\n",
      "\n",
      "  I doubt there are good prospects for  a self armoring system\n",
      "for venus surface conditions (several hundred degrees, very high\n",
      "pressure of CO2, possibly sulfuric and nitric acids or oxides\n",
      "but it is a notion to consider for outer planets rs where you might\n",
      "pick up ices under less extream upper atmosphere conditions buying\n",
      "deeper penetration.  A nice creative idea, unlikly but worthy of\n",
      "thinking about.\n",
      "\n",
      "index: 1\n",
      "label_id: 1\n",
      "file_data:\n",
      " From: ab@nova.cc.purdue.edu (Allen B)\n",
      "Subject: Re: TIFF: philosophical significance of 42\n",
      "Organization: Purdue University\n",
      "Lines: 39\n",
      "\n",
      "In article <prestonm.735400848@cs.man.ac.uk> prestonm@cs.man.ac.uk (Martin  \n",
      "Preston) writes:\n",
      "> Why not use the PD C library for reading/writing TIFF files? It took me a\n",
      "> good 20 minutes to start using them in your own app.\n",
      "\n",
      "I certainly do use it whenever I have to do TIFF, and it usually works\n",
      "very well.  That's not my point.  I'm >philosophically< opposed to it\n",
      "because of its complexity.\n",
      "\n",
      "This complexity has led to some programs' poor TIFF writers making\n",
      "some very bizarre files, other programs' inability to load TIFF\n",
      "images (though they'll save them, of course), and a general\n",
      "inability to interchange images between different environments\n",
      "despite the fact they all think they understand TIFF.\n",
      "\n",
      "As the saying goes, \"It's not me I'm worried about- it's all the\n",
      ">other<  assholes out there!\"  I've had big trouble with misuse and\n",
      "abuse of TIFF over the years, and I chalk it all up to the immense (and\n",
      "unnecessary) complexity of the format.\n",
      "\n",
      "In the words of the TIFF 5.0 spec, Appendix G, page G-1 (capitalized\n",
      "emphasis mine):\n",
      "\n",
      "\"The only problem with this sort of success is that TIFF was designed\n",
      "to be powerful and flexible, at the expense of simplicity.  It takes a\n",
      "fair amount of effort to handle all the options currently defined in\n",
      "this specification (PROBABLY NO APPLICATION DOES A COMPLETE JOB),\n",
      "and that is currently the only way you can be >sure< that you will be\n",
      "able to import any TIFF image, since there are so many\n",
      "image-generating applications out there now.\"\n",
      "\n",
      "\n",
      "If a program (or worse all applications) can't read >every< TIFF\n",
      "image, that means there are some it won't- some that I might have to\n",
      "deal with.  Why would I want my images to be trapped in that format?  I\n",
      "don't and neither should anyone who agrees with my reasoning- not\n",
      "that anyone does, of course! :-)\n",
      "\n",
      "ab\n",
      "\n",
      "index: 2\n",
      "label_id: 0\n",
      "file_data:\n",
      " From: healta@saturn.wwc.edu (Tammy R Healy)\n",
      "Subject: Cannanite genocide in the Bible\n",
      "Lines: 6\n",
      "Organization: Walla Walla College\n",
      "Lines: 6\n",
      "\n",
      "excuse me for my ignorance. But I remember reading once that the \n",
      "Biblical tribe known as the Philistines still exists...they are the modern \n",
      "day Palestinians.\n",
      "Anyone out there with more info, please post it!!!\n",
      "\n",
      "Tammy\n",
      "\n",
      "index: 3\n",
      "label_id: 1\n",
      "file_data:\n",
      " From: capelli@vnet.IBM.COM (Ron Capelli)\n",
      "Subject: Re: detecting double points in bezier curves\n",
      "Disclaimer: This posting represents the poster's views, not those of IBM\n",
      "News-Software: UReply 3.1\n",
      "Lines: 16\n",
      "\n",
      "In <ia522B1w165w@oeinck.waterland.wlink.nl> Ferdinand Oeinck writes:\n",
      ">I'm looking for any information on detecting and/or calculating a double\n",
      ">point and/or cusp in a bezier curve.\n",
      "\n",
      "See:\n",
      "   Maureen Stone and Tony DeRose,\n",
      "   \"A Geometric Characterization of Parametric Cubic Curves\",\n",
      "   ACM TOG, vol 8, no 3, July 1989, pp. 147-163.\n",
      "_______________________________________________________________________\n",
      "\n",
      "...Ron Capelli                 IBM Corp.  Dept. C13,  MS. P230\n",
      "   capelli@vnet.ibm.com        PO Box 950\n",
      "   (914) 435-1673              Poughkeepsie, NY  12602\n",
      "_______________________________________________________________________\n",
      "\n",
      "\"There are no answers, only cross references.\"\n",
      "\n",
      "index: 4\n",
      "label_id: 2\n",
      "file_data:\n",
      " From: henry@zoo.toronto.edu (Henry Spencer)\n",
      "Subject: Re: TRUE \"GLOBE\", Who makes it?\n",
      "Organization: U of Toronto Zoology\n",
      "Lines: 12\n",
      "\n",
      "In article <bill.047m@xpresso.UUCP> bill@xpresso.UUCP (Bill Vance) writes:\n",
      ">It has been known for quite a while that the earth is actually more pear\n",
      ">shaped than globular/spherical.  Does anyone make a \"globe\" that is accurate\n",
      ">as to actual shape, landmass configuration/Long/Lat lines etc.?\n",
      "\n",
      "I don't think you're going to be able to see the differences from a sphere\n",
      "unless they are greatly exaggerated.  Even the equatorial bulge is only\n",
      "about 1 part in 300 -- you'd never notice a 1mm error in a 30cm globe --\n",
      "and the other deviations from spherical shape are much smaller.\n",
      "-- \n",
      "SVR4 resembles a high-speed collision   | Henry Spencer @ U of Toronto Zoology\n",
      "between SVR3 and SunOS.    - Dick Dunn  |  henry@zoo.toronto.edu  utzoo!henry\n",
      "\n",
      "index: 5\n",
      "label_id: 2\n",
      "file_data:\n",
      " From: dietz@cs.rochester.edu (Paul Dietz)\n",
      "Subject: Commercial mining activities on the moon\n",
      "Organization: University of Rochester\n",
      "Lines: 38\n",
      "\n",
      "In article <1993Apr20.152819.28186@ke4zv.uucp> gary@ke4zv.UUCP (Gary Coffman) writes:\n",
      "\n",
      " > be the site of major commercial activity. As far as we know it has no\n",
      " > materials we can't get cheaper right here on Earth or from asteroids\n",
      " > and comets, aside from the semi-mythic He3 that *might* be useful in low\n",
      " > grade fusion reactors.\n",
      "\n",
      "I don't know what a \"low grade\" fusion reactor is, but the major\n",
      "problem with 3He (aside from the difficulty in making any fusion\n",
      "reactor work) is that its concentration in lunar regolith is just so\n",
      "small -- on the order of 5 ppb or so, on average (more in some\n",
      "fractions, but still very small).  Massive amounts of regolith would\n",
      "have to be processed.\n",
      "\n",
      "This thread reminds me of Wingo's claims some time ago about the moon\n",
      "as a source of titanium for use on earth.  As I recall, Wingo wasn't\n",
      "content with being assured that titanium (at .5% in the Earth's crust,\n",
      "average) would not run out, and touted lunar mines, even though the\n",
      "market price of ilmenite concentrate these days is around $.06/pound.\n",
      "This prompted me to look up large potential terrestrial sources.\n",
      "\n",
      "On the moon, titanium occurs in basalts; \"high-Ti\" basalts (Apollo 11\n",
      "and 17) have 8-14% titanium dioxide (by weight).  This is nice, but...\n",
      "terrestrial continental flood basalts are also typically enriched in\n",
      "titanium.  They very often have 3% TiO2, frequently have 4%, and\n",
      "sometimes even 5% TiO2 (again, by weight).  These flood basalts are\n",
      "*enormous* -- millions of cubic kilometers, scattered all over the\n",
      "world (Siberia, Brazil, the NW United States, Ethiopia, etc.).  If\n",
      "even 1% of the basalts are 5% TiO2, this is trillions of tons of TiO2\n",
      "at concentrations only a factor of 2-3 less than in lunar high-Ti\n",
      "basalts.  It is difficult to see how the disadvantages of the moon\n",
      "could be overcome by such a small increase the concentration of the\n",
      "ore (never mind the richer, but less common, terrestrial ores being\n",
      "mined today).\n",
      "\n",
      "\tPaul F. Dietz\n",
      "\tdietz@cs.rochester.edu\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for index in range(len(data)):\n",
    "    print(\"index:\", index)\n",
    "    label_id = data.target[index]\n",
    "    print(\"label_id:\",label_id)\n",
    "    file_data = data.data[index]\n",
    "    print(\"file_data:\\n\",file_data)\n",
    "    #i = file_data.find('\\n\\n')  # skip header\n",
    "    #if i > 0:\n",
    "    #    file_data = file_data[i:]\n",
    "    #try:\n",
    "    #    curr_str = str(file_data)\n",
    "    #    sentence_list = curr_str.split('\\n')\n",
    "    #    for sentence in sentence_list:\n",
    "    #        sentence = (sentence.strip()).lower()\n",
    "    #        texts.append(sentence)\n",
    "    #        texts_w2v.append(sentence.split(' '))\n",
    "    #        labels.append(label_id)\n",
    "    #except:\n",
    "    #    None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 첫번째 데이터만 가져와서 처리 과정을 차근차근 살펴보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index: 0\n",
      "label_id: 2\n",
      "file_data:\n",
      " From: degroff@netcom.com (21012d)\n",
      "Subject: Re: Venus Lander for Venus Conditions.\n",
      "Organization: Netcom Online Communications Services (408-241-9760 login: guest)\n",
      "Lines: 8\n",
      "\n",
      "\n",
      "  I doubt there are good prospects for  a self armoring system\n",
      "for venus surface conditions (several hundred degrees, very high\n",
      "pressure of CO2, possibly sulfuric and nitric acids or oxides\n",
      "but it is a notion to consider for outer planets rs where you might\n",
      "pick up ices under less extream upper atmosphere conditions buying\n",
      "deeper penetration.  A nice creative idea, unlikly but worthy of\n",
      "thinking about.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for index in range(1) : #len(data)):\n",
    "    print(\"index:\", index)\n",
    "    label_id = data.target[index]\n",
    "    print(\"label_id:\",label_id)\n",
    "    file_data = data.data[index]\n",
    "    print(\"file_data:\\n\",file_data)\n",
    "    #i = file_data.find('\\n\\n')  # skip header\n",
    "    #if i > 0:\n",
    "    #    file_data = file_data[i:]\n",
    "    #try:\n",
    "    #    curr_str = str(file_data)\n",
    "    #    sentence_list = curr_str.split('\\n')\n",
    "    #    for sentence in sentence_list:\n",
    "    #        sentence = (sentence.strip()).lower()\n",
    "    #        texts.append(sentence)\n",
    "    #        texts_w2v.append(sentence.split(' '))\n",
    "    #        labels.append(label_id)\n",
    "    #except:\n",
    "    #    None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index: 0\n",
      "label_id: 2\n",
      "file_data:\n",
      " From: degroff@netcom.com (21012d)\n",
      "Subject: Re: Venus Lander for Venus Conditions.\n",
      "Organization: Netcom Online Communications Services (408-241-9760 login: guest)\n",
      "Lines: 8\n",
      "\n",
      "\n",
      "  I doubt there are good prospects for  a self armoring system\n",
      "for venus surface conditions (several hundred degrees, very high\n",
      "pressure of CO2, possibly sulfuric and nitric acids or oxides\n",
      "but it is a notion to consider for outer planets rs where you might\n",
      "pick up ices under less extream upper atmosphere conditions buying\n",
      "deeper penetration.  A nice creative idea, unlikly but worthy of\n",
      "thinking about.\n",
      "\n",
      "i: 170\n"
     ]
    }
   ],
   "source": [
    "for index in range(1) : #len(data)):\n",
    "    print(\"index:\", index)\n",
    "    label_id = data.target[index]\n",
    "    print(\"label_id:\",label_id)\n",
    "    file_data = data.data[index]\n",
    "    print(\"file_data:\\n\",file_data)\n",
    "    i = file_data.find('\\n\\n')  # skip header\n",
    "    print(\"i:\", i)\n",
    "    #if i > 0:\n",
    "    #    file_data = file_data[i:]\n",
    "    #try:\n",
    "    #    curr_str = str(file_data)\n",
    "    #    sentence_list = curr_str.split('\\n')\n",
    "    #    for sentence in sentence_list:\n",
    "    #        sentence = (sentence.strip()).lower()\n",
    "    #        texts.append(sentence)\n",
    "    #        texts_w2v.append(sentence.split(' '))\n",
    "    #        labels.append(label_id)\n",
    "    #except:\n",
    "    #    None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index: 0\n",
      "label_id: 2\n",
      "file_data:\n",
      " From: degroff@netcom.com (21012d)\n",
      "Subject: Re: Venus Lander for Venus Conditions.\n",
      "Organization: Netcom Online Communications Services (408-241-9760 login: guest)\n",
      "Lines: 8\n",
      "\n",
      "\n",
      "  I doubt there are good prospects for  a self armoring system\n",
      "for venus surface conditions (several hundred degrees, very high\n",
      "pressure of CO2, possibly sulfuric and nitric acids or oxides\n",
      "but it is a notion to consider for outer planets rs where you might\n",
      "pick up ices under less extream upper atmosphere conditions buying\n",
      "deeper penetration.  A nice creative idea, unlikly but worthy of\n",
      "thinking about.\n",
      "\n",
      "i: 170\n",
      "file_data(skip_heaer): \n",
      "\n",
      "\n",
      "  I doubt there are good prospects for  a self armoring system\n",
      "for venus surface conditions (several hundred degrees, very high\n",
      "pressure of CO2, possibly sulfuric and nitric acids or oxides\n",
      "but it is a notion to consider for outer planets rs where you might\n",
      "pick up ices under less extream upper atmosphere conditions buying\n",
      "deeper penetration.  A nice creative idea, unlikly but worthy of\n",
      "thinking about.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for index in range(1) : #len(data)):\n",
    "    print(\"index:\", index)\n",
    "    label_id = data.target[index]\n",
    "    print(\"label_id:\",label_id)\n",
    "    file_data = data.data[index]\n",
    "    print(\"file_data:\\n\",file_data)\n",
    "    i = file_data.find('\\n\\n')  # skip header\n",
    "    print(\"i:\", i)\n",
    "    if i > 0:\n",
    "        file_data = file_data[i:]\n",
    "        print(\"file_data(skip_heaer):\", file_data)\n",
    "    #try:\n",
    "    #    curr_str = str(file_data)\n",
    "    #    sentence_list = curr_str.split('\\n')\n",
    "    #    for sentence in sentence_list:\n",
    "    #        sentence = (sentence.strip()).lower()\n",
    "    #        texts.append(sentence)\n",
    "    #        texts_w2v.append(sentence.split(' '))\n",
    "    #        labels.append(label_id)\n",
    "    #except:\n",
    "    #    None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curr_str \n",
      "\n",
      "\n",
      "  I doubt there are good prospects for  a self armoring system\n",
      "for venus surface conditions (several hundred degrees, very high\n",
      "pressure of CO2, possibly sulfuric and nitric acids or oxides\n",
      "but it is a notion to consider for outer planets rs where you might\n",
      "pick up ices under less extream upper atmosphere conditions buying\n",
      "deeper penetration.  A nice creative idea, unlikly but worthy of\n",
      "thinking about.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for index in range(1) : #len(data)):\n",
    "    #print(\"index:\", index)\n",
    "    label_id = data.target[index]\n",
    "    #print(\"label_id:\",label_id)\n",
    "    file_data = data.data[index]\n",
    "    #print(\"file_data:\\n\",file_data)\n",
    "    i = file_data.find('\\n\\n')  # skip header\n",
    "    #print(\"i:\", i)\n",
    "    if i > 0:\n",
    "        file_data = file_data[i:]\n",
    "        #print(\"file_data(skip_heaer):\", file_data)\n",
    "    try:\n",
    "        curr_str = str(file_data)\n",
    "        print(\"curr_str\", curr_str)\n",
    "    #    sentence_list = curr_str.split('\\n')\n",
    "    #    for sentence in sentence_list:\n",
    "    #        sentence = (sentence.strip()).lower()\n",
    "    #        texts.append(sentence)\n",
    "    #        texts_w2v.append(sentence.split(' '))\n",
    "    #        labels.append(label_id)\n",
    "    except:\n",
    "        None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curr_str \n",
      "\n",
      "\n",
      "  I doubt there are good prospects for  a self armoring system\n",
      "for venus surface conditions (several hundred degrees, very high\n",
      "pressure of CO2, possibly sulfuric and nitric acids or oxides\n",
      "but it is a notion to consider for outer planets rs where you might\n",
      "pick up ices under less extream upper atmosphere conditions buying\n",
      "deeper penetration.  A nice creative idea, unlikly but worthy of\n",
      "thinking about.\n",
      "\n",
      "sentence_list:\n",
      " ['', '', '', '  I doubt there are good prospects for  a self armoring system', 'for venus surface conditions (several hundred degrees, very high', 'pressure of CO2, possibly sulfuric and nitric acids or oxides', 'but it is a notion to consider for outer planets rs where you might', 'pick up ices under less extream upper atmosphere conditions buying', 'deeper penetration.  A nice creative idea, unlikly but worthy of', 'thinking about.', '']\n"
     ]
    }
   ],
   "source": [
    "for index in range(1) : #len(data)):\n",
    "    #print(\"index:\", index)\n",
    "    label_id = data.target[index]\n",
    "    #print(\"label_id:\",label_id)\n",
    "    file_data = data.data[index]\n",
    "    #print(\"file_data:\\n\",file_data)\n",
    "    i = file_data.find('\\n\\n')  # skip header\n",
    "    #print(\"i:\", i)\n",
    "    if i > 0:\n",
    "        file_data = file_data[i:]\n",
    "        #print(\"file_data(skip_heaer):\", file_data)\n",
    "    try:\n",
    "        curr_str = str(file_data)\n",
    "        print(\"curr_str\", curr_str)\n",
    "        sentence_list = curr_str.split('\\n')\n",
    "        print(\"sentence_list:\\n\", sentence_list)\n",
    "    #    for sentence in sentence_list:\n",
    "    #        sentence = (sentence.strip()).lower()\n",
    "    #        texts.append(sentence)\n",
    "    #        texts_w2v.append(sentence.split(' '))\n",
    "    #        labels.append(label_id)\n",
    "    except:\n",
    "        None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curr_str \n",
      "\n",
      "\n",
      "  I doubt there are good prospects for  a self armoring system\n",
      "for venus surface conditions (several hundred degrees, very high\n",
      "pressure of CO2, possibly sulfuric and nitric acids or oxides\n",
      "but it is a notion to consider for outer planets rs where you might\n",
      "pick up ices under less extream upper atmosphere conditions buying\n",
      "deeper penetration.  A nice creative idea, unlikly but worthy of\n",
      "thinking about.\n",
      "\n",
      "sentence_list:\n",
      " ['', '', '', '  I doubt there are good prospects for  a self armoring system', 'for venus surface conditions (several hundred degrees, very high', 'pressure of CO2, possibly sulfuric and nitric acids or oxides', 'but it is a notion to consider for outer planets rs where you might', 'pick up ices under less extream upper atmosphere conditions buying', 'deeper penetration.  A nice creative idea, unlikly but worthy of', 'thinking about.', '']\n",
      "sentence:\n",
      " \n",
      "sentence:\n",
      " \n",
      "sentence:\n",
      " \n",
      "sentence:\n",
      " i doubt there are good prospects for  a self armoring system\n",
      "sentence:\n",
      " for venus surface conditions (several hundred degrees, very high\n",
      "sentence:\n",
      " pressure of co2, possibly sulfuric and nitric acids or oxides\n",
      "sentence:\n",
      " but it is a notion to consider for outer planets rs where you might\n",
      "sentence:\n",
      " pick up ices under less extream upper atmosphere conditions buying\n",
      "sentence:\n",
      " deeper penetration.  a nice creative idea, unlikly but worthy of\n",
      "sentence:\n",
      " thinking about.\n",
      "sentence:\n",
      " \n"
     ]
    }
   ],
   "source": [
    "for index in range(1) : #len(data)):\n",
    "    #print(\"index:\", index)\n",
    "    label_id = data.target[index]\n",
    "    #print(\"label_id:\",label_id)\n",
    "    file_data = data.data[index]\n",
    "    #print(\"file_data:\\n\",file_data)\n",
    "    i = file_data.find('\\n\\n')  # skip header\n",
    "    #print(\"i:\", i)\n",
    "    if i > 0:\n",
    "        file_data = file_data[i:]\n",
    "        #print(\"file_data(skip_heaer):\", file_data)\n",
    "    try:\n",
    "        curr_str = str(file_data)\n",
    "        print(\"curr_str\", curr_str)\n",
    "        sentence_list = curr_str.split('\\n')\n",
    "        print(\"sentence_list:\\n\", sentence_list)\n",
    "        for sentence in sentence_list:\n",
    "            sentence = (sentence.strip()).lower()\n",
    "            print(\"sentence:\\n\", sentence)\n",
    "    #        texts.append(sentence)\n",
    "    #        texts_w2v.append(sentence.split(' '))\n",
    "    #        labels.append(label_id)\n",
    "    except:\n",
    "        None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence:\n",
      " \n",
      "sentence:\n",
      " \n",
      "sentence:\n",
      " \n",
      "sentence:\n",
      " i doubt there are good prospects for  a self armoring system\n",
      "sentence:\n",
      " for venus surface conditions (several hundred degrees, very high\n",
      "sentence:\n",
      " pressure of co2, possibly sulfuric and nitric acids or oxides\n",
      "sentence:\n",
      " but it is a notion to consider for outer planets rs where you might\n",
      "sentence:\n",
      " pick up ices under less extream upper atmosphere conditions buying\n",
      "sentence:\n",
      " deeper penetration.  a nice creative idea, unlikly but worthy of\n",
      "sentence:\n",
      " thinking about.\n",
      "sentence:\n",
      " \n",
      "texts:\n",
      " ['', '', '', 'i doubt there are good prospects for  a self armoring system', 'for venus surface conditions (several hundred degrees, very high', 'pressure of co2, possibly sulfuric and nitric acids or oxides', 'but it is a notion to consider for outer planets rs where you might', 'pick up ices under less extream upper atmosphere conditions buying', 'deeper penetration.  a nice creative idea, unlikly but worthy of', 'thinking about.', '']\n",
      "texts_w2v:\n",
      " []\n",
      "labels:\n",
      " []\n"
     ]
    }
   ],
   "source": [
    "texts = []  # list of text samples\n",
    "texts_w2v = []  # used to train the word embeddings\n",
    "labels = []  # list of label ids\n",
    "\n",
    "for index in range(1) : #len(data)):\n",
    "    #print(\"index:\", index)\n",
    "    label_id = data.target[index]\n",
    "    #print(\"label_id:\",label_id)\n",
    "    file_data = data.data[index]\n",
    "    #print(\"file_data:\\n\",file_data)\n",
    "    i = file_data.find('\\n\\n')  # skip header\n",
    "    #print(\"i:\", i)\n",
    "    if i > 0:\n",
    "        file_data = file_data[i:]\n",
    "        #print(\"file_data(skip_heaer):\", file_data)\n",
    "    try:\n",
    "        curr_str = str(file_data)\n",
    "        #print(\"curr_str\", curr_str)\n",
    "        sentence_list = curr_str.split('\\n')\n",
    "        #print(\"sentence_list:\\n\", sentence_list)\n",
    "        for sentence in sentence_list:\n",
    "            sentence = (sentence.strip()).lower()\n",
    "            print(\"sentence:\\n\", sentence)\n",
    "            texts.append(sentence)\n",
    "    #        texts_w2v.append(sentence.split(' '))\n",
    "    #        labels.append(label_id)\n",
    "    except:\n",
    "        None\n",
    "\n",
    "print(\"texts:\\n\", texts)\n",
    "print(\"texts_w2v:\\n\", texts_w2v)\n",
    "print(\"labels:\\n\", labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "texts:\n",
      " ['', '', '', 'i doubt there are good prospects for  a self armoring system', 'for venus surface conditions (several hundred degrees, very high', 'pressure of co2, possibly sulfuric and nitric acids or oxides', 'but it is a notion to consider for outer planets rs where you might', 'pick up ices under less extream upper atmosphere conditions buying', 'deeper penetration.  a nice creative idea, unlikly but worthy of', 'thinking about.', '']\n",
      "texts_w2v:\n",
      " [[''], [''], [''], ['i', 'doubt', 'there', 'are', 'good', 'prospects', 'for', '', 'a', 'self', 'armoring', 'system'], ['for', 'venus', 'surface', 'conditions', '(several', 'hundred', 'degrees,', 'very', 'high'], ['pressure', 'of', 'co2,', 'possibly', 'sulfuric', 'and', 'nitric', 'acids', 'or', 'oxides'], ['but', 'it', 'is', 'a', 'notion', 'to', 'consider', 'for', 'outer', 'planets', 'rs', 'where', 'you', 'might'], ['pick', 'up', 'ices', 'under', 'less', 'extream', 'upper', 'atmosphere', 'conditions', 'buying'], ['deeper', 'penetration.', '', 'a', 'nice', 'creative', 'idea,', 'unlikly', 'but', 'worthy', 'of'], ['thinking', 'about.'], ['']]\n",
      "labels:\n",
      " []\n"
     ]
    }
   ],
   "source": [
    "texts = []  # list of text samples\n",
    "texts_w2v = []  # used to train the word embeddings\n",
    "labels = []  # list of label ids\n",
    "\n",
    "for index in range(1) : #len(data)):\n",
    "    #print(\"index:\", index)\n",
    "    label_id = data.target[index]\n",
    "    #print(\"label_id:\",label_id)\n",
    "    file_data = data.data[index]\n",
    "    #print(\"file_data:\\n\",file_data)\n",
    "    i = file_data.find('\\n\\n')  # skip header\n",
    "    #print(\"i:\", i)\n",
    "    if i > 0:\n",
    "        file_data = file_data[i:]\n",
    "        #print(\"file_data(skip_heaer):\", file_data)\n",
    "    try:\n",
    "        curr_str = str(file_data)\n",
    "        #print(\"curr_str\", curr_str)\n",
    "        sentence_list = curr_str.split('\\n')\n",
    "        #print(\"sentence_list:\\n\", sentence_list)\n",
    "        for sentence in sentence_list:\n",
    "            sentence = (sentence.strip()).lower()\n",
    "            #print(\"sentence:\\n\", sentence)\n",
    "            texts.append(sentence)\n",
    "            texts_w2v.append(sentence.split(' '))\n",
    "    #        labels.append(label_id)\n",
    "    except:\n",
    "        None\n",
    "\n",
    "print(\"texts:\\n\", texts)\n",
    "print(\"texts_w2v:\\n\", texts_w2v)\n",
    "print(\"labels:\\n\", labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "texts:\n",
      " ['', '', '', 'i doubt there are good prospects for  a self armoring system', 'for venus surface conditions (several hundred degrees, very high', 'pressure of co2, possibly sulfuric and nitric acids or oxides', 'but it is a notion to consider for outer planets rs where you might', 'pick up ices under less extream upper atmosphere conditions buying', 'deeper penetration.  a nice creative idea, unlikly but worthy of', 'thinking about.', '']\n",
      "texts_w2v:\n",
      " [[''], [''], [''], ['i', 'doubt', 'there', 'are', 'good', 'prospects', 'for', '', 'a', 'self', 'armoring', 'system'], ['for', 'venus', 'surface', 'conditions', '(several', 'hundred', 'degrees,', 'very', 'high'], ['pressure', 'of', 'co2,', 'possibly', 'sulfuric', 'and', 'nitric', 'acids', 'or', 'oxides'], ['but', 'it', 'is', 'a', 'notion', 'to', 'consider', 'for', 'outer', 'planets', 'rs', 'where', 'you', 'might'], ['pick', 'up', 'ices', 'under', 'less', 'extream', 'upper', 'atmosphere', 'conditions', 'buying'], ['deeper', 'penetration.', '', 'a', 'nice', 'creative', 'idea,', 'unlikly', 'but', 'worthy', 'of'], ['thinking', 'about.'], ['']]\n",
      "labels:\n",
      " [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n"
     ]
    }
   ],
   "source": [
    "texts = []  # list of text samples\n",
    "texts_w2v = []  # used to train the word embeddings\n",
    "labels = []  # list of label ids\n",
    "\n",
    "for index in range(1) : #len(data)):\n",
    "    #print(\"index:\", index)\n",
    "    label_id = data.target[index]\n",
    "    #print(\"label_id:\",label_id)\n",
    "    file_data = data.data[index]\n",
    "    #print(\"file_data:\\n\",file_data)\n",
    "    i = file_data.find('\\n\\n')  # skip header\n",
    "    #print(\"i:\", i)\n",
    "    if i > 0:\n",
    "        file_data = file_data[i:]\n",
    "        #print(\"file_data(skip_heaer):\", file_data)\n",
    "    try:\n",
    "        curr_str = str(file_data)\n",
    "        #print(\"curr_str\", curr_str)\n",
    "        sentence_list = curr_str.split('\\n')\n",
    "        #print(\"sentence_list:\\n\", sentence_list)\n",
    "        for sentence in sentence_list:\n",
    "            sentence = (sentence.strip()).lower()\n",
    "            #print(\"sentence:\\n\", sentence)\n",
    "            texts.append(sentence)\n",
    "            texts_w2v.append(sentence.split(' '))\n",
    "            labels.append(label_id)\n",
    "    except:\n",
    "        None\n",
    "\n",
    "print(\"texts:\\n\", texts)\n",
    "print(\"texts_w2v:\\n\", texts_w2v)\n",
    "print(\"labels:\\n\", labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 데이터 전체에 적용한 결과"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "texts:\n",
      " ['', '', '', 'i doubt there are good prospects for  a self armoring system', 'for venus surface conditions (several hundred degrees, very high', 'pressure of co2, possibly sulfuric and nitric acids or oxides', 'but it is a notion to consider for outer planets rs where you might', 'pick up ices under less extream upper atmosphere conditions buying', 'deeper penetration.  a nice creative idea, unlikly but worthy of', 'thinking about.', '', '', '', 'in article <prestonm.735400848@cs.man.ac.uk> prestonm@cs.man.ac.uk (martin', 'preston) writes:', '> why not use the pd c library for reading/writing tiff files? it took me a', '> good 20 minutes to start using them in your own app.', '', 'i certainly do use it whenever i have to do tiff, and it usually works', \"very well.  that's not my point.  i'm >philosophically< opposed to it\", 'because of its complexity.', '', \"this complexity has led to some programs' poor tiff writers making\", \"some very bizarre files, other programs' inability to load tiff\", \"images (though they'll save them, of course), and a general\", 'inability to interchange images between different environments', 'despite the fact they all think they understand tiff.', '', 'as the saying goes, \"it\\'s not me i\\'m worried about- it\\'s all the', '>other<  assholes out there!\"  i\\'ve had big trouble with misuse and', 'abuse of tiff over the years, and i chalk it all up to the immense (and', 'unnecessary) complexity of the format.', '', 'in the words of the tiff 5.0 spec, appendix g, page g-1 (capitalized', 'emphasis mine):', '', '\"the only problem with this sort of success is that tiff was designed', 'to be powerful and flexible, at the expense of simplicity.  it takes a', 'fair amount of effort to handle all the options currently defined in', 'this specification (probably no application does a complete job),', 'and that is currently the only way you can be >sure< that you will be', 'able to import any tiff image, since there are so many', 'image-generating applications out there now.\"', '', '', \"if a program (or worse all applications) can't read >every< tiff\", \"image, that means there are some it won't- some that i might have to\", 'deal with.  why would i want my images to be trapped in that format?  i', \"don't and neither should anyone who agrees with my reasoning- not\", 'that anyone does, of course! :-)', '', 'ab', '', '', '', 'excuse me for my ignorance. but i remember reading once that the', 'biblical tribe known as the philistines still exists...they are the modern', 'day palestinians.', 'anyone out there with more info, please post it!!!', '', 'tammy', '', '', '', 'in <ia522b1w165w@oeinck.waterland.wlink.nl> ferdinand oeinck writes:', \">i'm looking for any information on detecting and/or calculating a double\", '>point and/or cusp in a bezier curve.', '', 'see:', 'maureen stone and tony derose,', '\"a geometric characterization of parametric cubic curves\",', 'acm tog, vol 8, no 3, july 1989, pp. 147-163.', '_______________________________________________________________________', '', '...ron capelli                 ibm corp.  dept. c13,  ms. p230', 'capelli@vnet.ibm.com        po box 950', '(914) 435-1673              poughkeepsie, ny  12602', '_______________________________________________________________________', '', '\"there are no answers, only cross references.\"', '', '', '', 'in article <bill.047m@xpresso.uucp> bill@xpresso.uucp (bill vance) writes:', '>it has been known for quite a while that the earth is actually more pear', '>shaped than globular/spherical.  does anyone make a \"globe\" that is accurate', '>as to actual shape, landmass configuration/long/lat lines etc.?', '', \"i don't think you're going to be able to see the differences from a sphere\", 'unless they are greatly exaggerated.  even the equatorial bulge is only', \"about 1 part in 300 -- you'd never notice a 1mm error in a 30cm globe --\", 'and the other deviations from spherical shape are much smaller.', '--', 'svr4 resembles a high-speed collision   | henry spencer @ u of toronto zoology', 'between svr3 and sunos.    - dick dunn  |  henry@zoo.toronto.edu  utzoo!henry', '', '', '', 'in article <1993apr20.152819.28186@ke4zv.uucp> gary@ke4zv.uucp (gary coffman) writes:', '', '> be the site of major commercial activity. as far as we know it has no', \"> materials we can't get cheaper right here on earth or from asteroids\", '> and comets, aside from the semi-mythic he3 that *might* be useful in low', '> grade fusion reactors.', '', 'i don\\'t know what a \"low grade\" fusion reactor is, but the major', 'problem with 3he (aside from the difficulty in making any fusion', 'reactor work) is that its concentration in lunar regolith is just so', 'small -- on the order of 5 ppb or so, on average (more in some', 'fractions, but still very small).  massive amounts of regolith would', 'have to be processed.', '', \"this thread reminds me of wingo's claims some time ago about the moon\", \"as a source of titanium for use on earth.  as i recall, wingo wasn't\", \"content with being assured that titanium (at .5% in the earth's crust,\", 'average) would not run out, and touted lunar mines, even though the', 'market price of ilmenite concentrate these days is around $.06/pound.', 'this prompted me to look up large potential terrestrial sources.', '', 'on the moon, titanium occurs in basalts; \"high-ti\" basalts (apollo 11', 'and 17) have 8-14% titanium dioxide (by weight).  this is nice, but...', 'terrestrial continental flood basalts are also typically enriched in', 'titanium.  they very often have 3% tio2, frequently have 4%, and', 'sometimes even 5% tio2 (again, by weight).  these flood basalts are', '*enormous* -- millions of cubic kilometers, scattered all over the', 'world (siberia, brazil, the nw united states, ethiopia, etc.).  if', 'even 1% of the basalts are 5% tio2, this is trillions of tons of tio2', 'at concentrations only a factor of 2-3 less than in lunar high-ti', 'basalts.  it is difficult to see how the disadvantages of the moon', 'could be overcome by such a small increase the concentration of the', 'ore (never mind the richer, but less common, terrestrial ores being', 'mined today).', '', 'paul f. dietz', 'dietz@cs.rochester.edu', '', '']\n",
      "texts_w2v:\n",
      " [[''], [''], [''], ['i', 'doubt', 'there', 'are', 'good', 'prospects', 'for', '', 'a', 'self', 'armoring', 'system'], ['for', 'venus', 'surface', 'conditions', '(several', 'hundred', 'degrees,', 'very', 'high'], ['pressure', 'of', 'co2,', 'possibly', 'sulfuric', 'and', 'nitric', 'acids', 'or', 'oxides'], ['but', 'it', 'is', 'a', 'notion', 'to', 'consider', 'for', 'outer', 'planets', 'rs', 'where', 'you', 'might'], ['pick', 'up', 'ices', 'under', 'less', 'extream', 'upper', 'atmosphere', 'conditions', 'buying'], ['deeper', 'penetration.', '', 'a', 'nice', 'creative', 'idea,', 'unlikly', 'but', 'worthy', 'of'], ['thinking', 'about.'], [''], [''], [''], ['in', 'article', '<prestonm.735400848@cs.man.ac.uk>', 'prestonm@cs.man.ac.uk', '(martin'], ['preston)', 'writes:'], ['>', 'why', 'not', 'use', 'the', 'pd', 'c', 'library', 'for', 'reading/writing', 'tiff', 'files?', 'it', 'took', 'me', 'a'], ['>', 'good', '20', 'minutes', 'to', 'start', 'using', 'them', 'in', 'your', 'own', 'app.'], [''], ['i', 'certainly', 'do', 'use', 'it', 'whenever', 'i', 'have', 'to', 'do', 'tiff,', 'and', 'it', 'usually', 'works'], ['very', 'well.', '', \"that's\", 'not', 'my', 'point.', '', \"i'm\", '>philosophically<', 'opposed', 'to', 'it'], ['because', 'of', 'its', 'complexity.'], [''], ['this', 'complexity', 'has', 'led', 'to', 'some', \"programs'\", 'poor', 'tiff', 'writers', 'making'], ['some', 'very', 'bizarre', 'files,', 'other', \"programs'\", 'inability', 'to', 'load', 'tiff'], ['images', '(though', \"they'll\", 'save', 'them,', 'of', 'course),', 'and', 'a', 'general'], ['inability', 'to', 'interchange', 'images', 'between', 'different', 'environments'], ['despite', 'the', 'fact', 'they', 'all', 'think', 'they', 'understand', 'tiff.'], [''], ['as', 'the', 'saying', 'goes,', '\"it\\'s', 'not', 'me', \"i'm\", 'worried', 'about-', \"it's\", 'all', 'the'], ['>other<', '', 'assholes', 'out', 'there!\"', '', \"i've\", 'had', 'big', 'trouble', 'with', 'misuse', 'and'], ['abuse', 'of', 'tiff', 'over', 'the', 'years,', 'and', 'i', 'chalk', 'it', 'all', 'up', 'to', 'the', 'immense', '(and'], ['unnecessary)', 'complexity', 'of', 'the', 'format.'], [''], ['in', 'the', 'words', 'of', 'the', 'tiff', '5.0', 'spec,', 'appendix', 'g,', 'page', 'g-1', '(capitalized'], ['emphasis', 'mine):'], [''], ['\"the', 'only', 'problem', 'with', 'this', 'sort', 'of', 'success', 'is', 'that', 'tiff', 'was', 'designed'], ['to', 'be', 'powerful', 'and', 'flexible,', 'at', 'the', 'expense', 'of', 'simplicity.', '', 'it', 'takes', 'a'], ['fair', 'amount', 'of', 'effort', 'to', 'handle', 'all', 'the', 'options', 'currently', 'defined', 'in'], ['this', 'specification', '(probably', 'no', 'application', 'does', 'a', 'complete', 'job),'], ['and', 'that', 'is', 'currently', 'the', 'only', 'way', 'you', 'can', 'be', '>sure<', 'that', 'you', 'will', 'be'], ['able', 'to', 'import', 'any', 'tiff', 'image,', 'since', 'there', 'are', 'so', 'many'], ['image-generating', 'applications', 'out', 'there', 'now.\"'], [''], [''], ['if', 'a', 'program', '(or', 'worse', 'all', 'applications)', \"can't\", 'read', '>every<', 'tiff'], ['image,', 'that', 'means', 'there', 'are', 'some', 'it', \"won't-\", 'some', 'that', 'i', 'might', 'have', 'to'], ['deal', 'with.', '', 'why', 'would', 'i', 'want', 'my', 'images', 'to', 'be', 'trapped', 'in', 'that', 'format?', '', 'i'], [\"don't\", 'and', 'neither', 'should', 'anyone', 'who', 'agrees', 'with', 'my', 'reasoning-', 'not'], ['that', 'anyone', 'does,', 'of', 'course!', ':-)'], [''], ['ab'], [''], [''], [''], ['excuse', 'me', 'for', 'my', 'ignorance.', 'but', 'i', 'remember', 'reading', 'once', 'that', 'the'], ['biblical', 'tribe', 'known', 'as', 'the', 'philistines', 'still', 'exists...they', 'are', 'the', 'modern'], ['day', 'palestinians.'], ['anyone', 'out', 'there', 'with', 'more', 'info,', 'please', 'post', 'it!!!'], [''], ['tammy'], [''], [''], [''], ['in', '<ia522b1w165w@oeinck.waterland.wlink.nl>', 'ferdinand', 'oeinck', 'writes:'], [\">i'm\", 'looking', 'for', 'any', 'information', 'on', 'detecting', 'and/or', 'calculating', 'a', 'double'], ['>point', 'and/or', 'cusp', 'in', 'a', 'bezier', 'curve.'], [''], ['see:'], ['maureen', 'stone', 'and', 'tony', 'derose,'], ['\"a', 'geometric', 'characterization', 'of', 'parametric', 'cubic', 'curves\",'], ['acm', 'tog,', 'vol', '8,', 'no', '3,', 'july', '1989,', 'pp.', '147-163.'], ['_______________________________________________________________________'], [''], ['...ron', 'capelli', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', 'ibm', 'corp.', '', 'dept.', 'c13,', '', 'ms.', 'p230'], ['capelli@vnet.ibm.com', '', '', '', '', '', '', '', 'po', 'box', '950'], ['(914)', '435-1673', '', '', '', '', '', '', '', '', '', '', '', '', '', 'poughkeepsie,', 'ny', '', '12602'], ['_______________________________________________________________________'], [''], ['\"there', 'are', 'no', 'answers,', 'only', 'cross', 'references.\"'], [''], [''], [''], ['in', 'article', '<bill.047m@xpresso.uucp>', 'bill@xpresso.uucp', '(bill', 'vance)', 'writes:'], ['>it', 'has', 'been', 'known', 'for', 'quite', 'a', 'while', 'that', 'the', 'earth', 'is', 'actually', 'more', 'pear'], ['>shaped', 'than', 'globular/spherical.', '', 'does', 'anyone', 'make', 'a', '\"globe\"', 'that', 'is', 'accurate'], ['>as', 'to', 'actual', 'shape,', 'landmass', 'configuration/long/lat', 'lines', 'etc.?'], [''], ['i', \"don't\", 'think', \"you're\", 'going', 'to', 'be', 'able', 'to', 'see', 'the', 'differences', 'from', 'a', 'sphere'], ['unless', 'they', 'are', 'greatly', 'exaggerated.', '', 'even', 'the', 'equatorial', 'bulge', 'is', 'only'], ['about', '1', 'part', 'in', '300', '--', \"you'd\", 'never', 'notice', 'a', '1mm', 'error', 'in', 'a', '30cm', 'globe', '--'], ['and', 'the', 'other', 'deviations', 'from', 'spherical', 'shape', 'are', 'much', 'smaller.'], ['--'], ['svr4', 'resembles', 'a', 'high-speed', 'collision', '', '', '|', 'henry', 'spencer', '@', 'u', 'of', 'toronto', 'zoology'], ['between', 'svr3', 'and', 'sunos.', '', '', '', '-', 'dick', 'dunn', '', '|', '', 'henry@zoo.toronto.edu', '', 'utzoo!henry'], [''], [''], [''], ['in', 'article', '<1993apr20.152819.28186@ke4zv.uucp>', 'gary@ke4zv.uucp', '(gary', 'coffman)', 'writes:'], [''], ['>', 'be', 'the', 'site', 'of', 'major', 'commercial', 'activity.', 'as', 'far', 'as', 'we', 'know', 'it', 'has', 'no'], ['>', 'materials', 'we', \"can't\", 'get', 'cheaper', 'right', 'here', 'on', 'earth', 'or', 'from', 'asteroids'], ['>', 'and', 'comets,', 'aside', 'from', 'the', 'semi-mythic', 'he3', 'that', '*might*', 'be', 'useful', 'in', 'low'], ['>', 'grade', 'fusion', 'reactors.'], [''], ['i', \"don't\", 'know', 'what', 'a', '\"low', 'grade\"', 'fusion', 'reactor', 'is,', 'but', 'the', 'major'], ['problem', 'with', '3he', '(aside', 'from', 'the', 'difficulty', 'in', 'making', 'any', 'fusion'], ['reactor', 'work)', 'is', 'that', 'its', 'concentration', 'in', 'lunar', 'regolith', 'is', 'just', 'so'], ['small', '--', 'on', 'the', 'order', 'of', '5', 'ppb', 'or', 'so,', 'on', 'average', '(more', 'in', 'some'], ['fractions,', 'but', 'still', 'very', 'small).', '', 'massive', 'amounts', 'of', 'regolith', 'would'], ['have', 'to', 'be', 'processed.'], [''], ['this', 'thread', 'reminds', 'me', 'of', \"wingo's\", 'claims', 'some', 'time', 'ago', 'about', 'the', 'moon'], ['as', 'a', 'source', 'of', 'titanium', 'for', 'use', 'on', 'earth.', '', 'as', 'i', 'recall,', 'wingo', \"wasn't\"], ['content', 'with', 'being', 'assured', 'that', 'titanium', '(at', '.5%', 'in', 'the', \"earth's\", 'crust,'], ['average)', 'would', 'not', 'run', 'out,', 'and', 'touted', 'lunar', 'mines,', 'even', 'though', 'the'], ['market', 'price', 'of', 'ilmenite', 'concentrate', 'these', 'days', 'is', 'around', '$.06/pound.'], ['this', 'prompted', 'me', 'to', 'look', 'up', 'large', 'potential', 'terrestrial', 'sources.'], [''], ['on', 'the', 'moon,', 'titanium', 'occurs', 'in', 'basalts;', '\"high-ti\"', 'basalts', '(apollo', '11'], ['and', '17)', 'have', '8-14%', 'titanium', 'dioxide', '(by', 'weight).', '', 'this', 'is', 'nice,', 'but...'], ['terrestrial', 'continental', 'flood', 'basalts', 'are', 'also', 'typically', 'enriched', 'in'], ['titanium.', '', 'they', 'very', 'often', 'have', '3%', 'tio2,', 'frequently', 'have', '4%,', 'and'], ['sometimes', 'even', '5%', 'tio2', '(again,', 'by', 'weight).', '', 'these', 'flood', 'basalts', 'are'], ['*enormous*', '--', 'millions', 'of', 'cubic', 'kilometers,', 'scattered', 'all', 'over', 'the'], ['world', '(siberia,', 'brazil,', 'the', 'nw', 'united', 'states,', 'ethiopia,', 'etc.).', '', 'if'], ['even', '1%', 'of', 'the', 'basalts', 'are', '5%', 'tio2,', 'this', 'is', 'trillions', 'of', 'tons', 'of', 'tio2'], ['at', 'concentrations', 'only', 'a', 'factor', 'of', '2-3', 'less', 'than', 'in', 'lunar', 'high-ti'], ['basalts.', '', 'it', 'is', 'difficult', 'to', 'see', 'how', 'the', 'disadvantages', 'of', 'the', 'moon'], ['could', 'be', 'overcome', 'by', 'such', 'a', 'small', 'increase', 'the', 'concentration', 'of', 'the'], ['ore', '(never', 'mind', 'the', 'richer,', 'but', 'less', 'common,', 'terrestrial', 'ores', 'being'], ['mined', 'today).'], [''], ['paul', 'f.', 'dietz'], ['dietz@cs.rochester.edu'], [''], ['']]\n",
      "labels:\n",
      " [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n"
     ]
    }
   ],
   "source": [
    "# full code \n",
    "\n",
    "texts = []  # list of text samples\n",
    "texts_w2v = []  # used to train the word embeddings\n",
    "labels = []  # list of label ids\n",
    "\n",
    "for index in range(len(data)):\n",
    "    label_id = data.target[index]\n",
    "    file_data = data.data[index]\n",
    "    i = file_data.find('\\n\\n')  # skip header\n",
    "    if i > 0:\n",
    "        file_data = file_data[i:]\n",
    "    try:\n",
    "        curr_str = str(file_data)\n",
    "        sentence_list = curr_str.split('\\n')\n",
    "        for sentence in sentence_list:\n",
    "            sentence = (sentence.strip()).lower()\n",
    "            texts.append(sentence)\n",
    "            texts_w2v.append(sentence.split(' '))\n",
    "            labels.append(label_id)\n",
    "    except:\n",
    "        None\n",
    "        \n",
    "        \n",
    "print(\"texts:\\n\", texts)\n",
    "print(\"texts_w2v:\\n\", texts_w2v)\n",
    "print(\"labels:\\n\", labels)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 준비 (한국어/실습)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 다음 문서들을 이용해서 데이터를 만들자. \n",
    "* https://gasazip.com/view.html?no=614736\n",
    "* https://gasazip.com/1224697\n",
    "* https://gasazip.com/view.html?no=599082\n",
    "* https://gasazip.com/view.html?no=645465\n",
    "* http://gasazip.com/view.html?no=643505\n",
    "* https://gasazip.com/view.html?no=615362    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "texts:\n",
      " []\n",
      "texts_w2v:\n",
      " []\n",
      "labels:\n",
      " []\n"
     ]
    }
   ],
   "source": [
    "# -- coding \n",
    "\n",
    "texts_kr = []  # list of text samples\n",
    "texts_w2v_kr = []  # used to train the word embeddings\n",
    "labels_kr = []  # list of label ids\n",
    "\n",
    "# -- 여기에 데이터 준비하는 코드를 만들면 된다.        \n",
    "        \n",
    "print(\"texts:\\n\", texts_kr)\n",
    "print(\"texts_w2v:\\n\", texts_w2v_kr)\n",
    "print(\"labels:\\n\", labels_kr)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 딥러닝 모델에 입력가능한 포맷으로 변환  (영어 - 20newsgroups)\n",
    "we format our text samples and labels into tensors that can be fed into a neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we rely on Keras utilities keras.preprocessing.text.Tokenizer and keras.preprocessing.sequence.pad_sequences.\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 1000\n",
    "\n",
    "# Vectorize the text samples into a 2D integer tensor\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "# word_index = tokenizer.word_index\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "labels = to_categorical(np.asarray(labels))\n",
    "\n",
    "x_train = data\n",
    "y_train = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "137"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[],\n",
       " [],\n",
       " [],\n",
       " [10, 141, 15, 11, 76, 142, 14, 3, 143, 144, 145],\n",
       " [14, 146, 147, 77, 148, 149, 150, 26, 34]]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "137"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0,   0,   0, ...,   0,   0,   0],\n",
       "       [  0,   0,   0, ...,   0,   0,   0],\n",
       "       [  0,   0,   0, ...,   0,   0,   0],\n",
       "       [  0,   0,   0, ..., 143, 144, 145],\n",
       "       [  0,   0,   0, ..., 150,  26,  34]], dtype=int32)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 딥러닝 모델에 입력가능한 포맷으로 변환 (한국어/실습)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -- coding\n",
    "\n",
    "#x_train_kr \n",
    "#y_train_kr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras에서 사용할 embedding layer 생성 (영어 - 20newsgroups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Keras_w2v = word2vec.Word2Vec(min_count=1)\n",
    "Keras_w2v.build_vocab(texts_w2v)\n",
    "Keras_w2v.train(texts, total_examples=Keras_w2v.corpus_count, epochs=Keras_w2v.iter)\n",
    "Keras_w2v_wv = Keras_w2v.wv\n",
    "embedding_layer = Keras_w2v_wv.get_keras_embedding()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras에서 사용할 embedding layer 생성 (한국어/실습)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -- coding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN 모델 생성 및 학습 (영어 - 20newsgroups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "137/137 [==============================] - 4s 30ms/step - loss: 0.9788 - acc: 0.4599\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 2/5\n",
      "137/137 [==============================] - 4s 28ms/step - loss: 0.9587 - acc: 0.4964\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 3/5\n",
      "137/137 [==============================] - 3s 20ms/step - loss: 0.8897 - acc: 0.4234\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 4/5\n",
      "137/137 [==============================] - 2s 16ms/step - loss: 0.8881 - acc: 0.5036\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 5/5\n",
      "137/137 [==============================] - 2s 17ms/step - loss: 0.8739 - acc: 0.4891\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fd7115ee3c8>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finally, we create a small 1D convnet to solve our classification problem.\n",
    "\n",
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "x = Conv1D(128, 5, activation='relu')(embedded_sequences)\n",
    "x = MaxPooling1D(5)(x)\n",
    "x = Conv1D(128, 5, activation='relu')(x)\n",
    "x = MaxPooling1D(5)(x)\n",
    "x = Conv1D(128, 5, activation='relu')(x)\n",
    "x = MaxPooling1D(35)(x)  # global max pooling\n",
    "x = Flatten()(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "preds = Dense(y_train.shape[1], activation='softmax')(x)\n",
    "\n",
    "model = Model(sequence_input, preds)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['acc'])\n",
    "\n",
    "model.fit(x_train, y_train, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN 모델 생성 및 학습 (한국어/실습)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -- coding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습된 모델로 새로운 문장(or 문서) 분류하기 (영어 - 20newsgroups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_text(text):\n",
    "    \"\"\" \n",
    "    Process the input text by tokenizing and padding it.\n",
    "    \"\"\"\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(text)\n",
    "    x = tokenizer.texts_to_sequences(text)\n",
    "\n",
    "    x = pad_sequences(x, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sci.space': 0.41577151, 'comp.graphics': 0.41070491, 'alt.atheism': 0.17352362}\n"
     ]
    }
   ],
   "source": [
    "input_text = 'artificial intelligence'\n",
    "\n",
    "matrix = process_text(input_text)\n",
    "\n",
    "predictions = model.predict(matrix)\n",
    "\n",
    "categories=['alt.atheism', 'comp.graphics', 'sci.space']\n",
    "# get the actual categories from output\n",
    "scoredict = {}\n",
    "for idx, classlabel in zip(range(len(categories)), categories):\n",
    "    scoredict[classlabel] = predictions[0][idx]\n",
    "\n",
    "print(scoredict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습된 모델로 새로운 문장(or 문서) 분류하기 (한국어/실습)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -- coding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2D CNN for Text (Gensim + Keras)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 참고\n",
    "* [3] https://github.com/bhaveshoswal/CNN-text-classification-keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense, Embedding, Conv2D, MaxPool2D\n",
    "from keras.layers import Reshape, Flatten, Dropout, Concatenate\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Model...\n",
      "Traning Model...\n",
      "Epoch 1/5\n",
      "137/137 [==============================] - 50s 362ms/step - loss: 0.6489 - acc: 0.6667\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 2/5\n",
      "137/137 [==============================] - 49s 360ms/step - loss: 0.6068 - acc: 0.6667\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 3/5\n",
      "137/137 [==============================] - 49s 361ms/step - loss: 0.5849 - acc: 0.6667\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 4/5\n",
      "137/137 [==============================] - 45s 330ms/step - loss: 0.5605 - acc: 0.6740\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "Epoch 5/5\n",
      "137/137 [==============================] - 54s 395ms/step - loss: 0.5454 - acc: 0.6740\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fd6c065f6a0>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence_length = MAX_SEQUENCE_LENGTH\n",
    "vocabulary_size = len(Keras_w2v_wv.vocab.items())\n",
    "embedding_dim = 256\n",
    "filter_sizes = [3,4,5]\n",
    "num_filters = 512\n",
    "drop = 0.5\n",
    "\n",
    "epochs = 100\n",
    "batch_size = 30\n",
    "\n",
    "# this returns a tensor\n",
    "print(\"Creating Model...\")\n",
    "inputs = Input(shape=(sequence_length,), dtype='int32')\n",
    "embedding = Embedding(input_dim=vocabulary_size, output_dim=embedding_dim, input_length=sequence_length)(inputs)\n",
    "reshape = Reshape((sequence_length,embedding_dim,1))(embedding)\n",
    "\n",
    "conv_0 = Conv2D(num_filters, kernel_size=(filter_sizes[0], embedding_dim), padding='valid', kernel_initializer='normal', activation='relu')(reshape)\n",
    "conv_1 = Conv2D(num_filters, kernel_size=(filter_sizes[1], embedding_dim), padding='valid', kernel_initializer='normal', activation='relu')(reshape)\n",
    "conv_2 = Conv2D(num_filters, kernel_size=(filter_sizes[2], embedding_dim), padding='valid', kernel_initializer='normal', activation='relu')(reshape)\n",
    "\n",
    "maxpool_0 = MaxPool2D(pool_size=(sequence_length - filter_sizes[0] + 1, 1), strides=(1,1), padding='valid')(conv_0)\n",
    "maxpool_1 = MaxPool2D(pool_size=(sequence_length - filter_sizes[1] + 1, 1), strides=(1,1), padding='valid')(conv_1)\n",
    "maxpool_2 = MaxPool2D(pool_size=(sequence_length - filter_sizes[2] + 1, 1), strides=(1,1), padding='valid')(conv_2)\n",
    "\n",
    "concatenated_tensor = Concatenate(axis=1)([maxpool_0, maxpool_1, maxpool_2])\n",
    "flatten = Flatten()(concatenated_tensor)\n",
    "dropout = Dropout(drop)(flatten)\n",
    "output = Dense(units=3, activation='softmax')(dropout)\n",
    "\n",
    "# this creates a model that includes\n",
    "model = Model(inputs=inputs, outputs=output)\n",
    "\n",
    "checkpoint = ModelCheckpoint('weights.{epoch:03d}-{val_acc:.4f}.hdf5', monitor='val_acc', verbose=1, save_best_only=True, mode='auto')\n",
    "adam = Adam(lr=1e-4, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "\n",
    "model.compile(optimizer=adam, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "print(\"Traning Model...\")\n",
    "model.fit(x_train, y_train, epochs=5) # batch_size=batch_size, epochs=epochs, verbose=1, callbacks=[checkpoint], validation_data=(X_test, y_test))  # starts training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sci.space': 0.43866351, 'comp.graphics': 0.40509725, 'alt.atheism': 0.15623923}\n"
     ]
    }
   ],
   "source": [
    "input_text = 'artificial intelligence'\n",
    "\n",
    "matrix = process_text(input_text)\n",
    "\n",
    "predictions = model.predict(matrix)\n",
    "\n",
    "categories=['alt.atheism', 'comp.graphics', 'sci.space']\n",
    "# get the actual categories from output\n",
    "scoredict = {}\n",
    "for idx, classlabel in zip(range(len(categories)), categories):\n",
    "    scoredict[classlabel] = predictions[0][idx]\n",
    "\n",
    "print(scoredict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 참고자료\n",
    "* [2] Using wrappers for Gensim models for working with Keras - https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/keras_wrapper.ipynb\n",
    "* [3] https://github.com/bhaveshoswal/CNN-text-classification-keras\n",
    "* [4] https://github.com/keras-team/keras/blob/ce406b773b9f36be5718a4369ad07fea4f9ebdba/examples/imdb_cnn.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "name": "Gensim Newsgroup.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
