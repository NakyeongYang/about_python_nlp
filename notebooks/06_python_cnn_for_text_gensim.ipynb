{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 06. 파이썬을 이용한 CNN for Text (Using Gensim + Keras)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "* 싸이그래머 / 어바웃 파이썬\n",
    "* 김무성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 차례\n",
    "* Word2Vec (Gensim) + Deep Learning (Keras) 간단 예제 : 코사도 유사도  \n",
    "* 1D CNN for Text (Gensim + Keras)\n",
    "* 2D CNN for Text (Gensim + Keras)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Word2Vec (Gensim) + Deep Learning (Keras) 간단 예제 : 코사도 유사도 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    ['human', 'interface', 'computer'],\n",
    "    ['survey', 'user', 'computer', 'system', 'response', 'time'],\n",
    "    ['eps', 'user', 'interface', 'system'],\n",
    "    ['system', 'human', 'system', 'eps'],\n",
    "    ['user', 'response', 'time'],\n",
    "    ['trees'],\n",
    "    ['graph', 'trees'],\n",
    "    ['graph', 'minors', 'trees'],\n",
    "    ['graph', 'minors', 'survey']\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model = word2vec.Word2Vec(sentences, size=100, min_count=1, hs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.engine import Input\n",
    "from keras.models import Model\n",
    "from keras.layers.merge import dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "wv = model.wv\n",
    "embedding_layer = wv.get_keras_embedding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "input_a = Input(shape=(1,), dtype='int32', name='input_a')\n",
    "input_b = Input(shape=(1,), dtype='int32', name='input_b')\n",
    "embedding_a = embedding_layer(input_a)\n",
    "embedding_b = embedding_layer(input_b)\n",
    "similarity = dot([embedding_a, embedding_b], axes=2, normalize=True)\n",
    "\n",
    "keras_model = Model(input=[input_a, input_b], output=similarity)\n",
    "keras_model.compile(optimizer='sgd', loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "word_a = 'graph'\n",
    "word_b = 'trees'\n",
    "# output is the cosine distance between the two words (as a similarity measure)\n",
    "output = keras_model.predict([np.asarray([model.wv.vocab[word_a].index]), np.asarray([model.wv.vocab[word_b].index])])\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "--------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 1D CNN for Text (Gensim + Keras) \n",
    "* Data 준비\n",
    "* 딥러닝 모델에 입력가능한 포맷으로 변환\n",
    "* Keras에서 사용할 embedding layer 생성\n",
    "* CNN 모델 생성 및 학습\n",
    "* 학습된 모델로 새로운 문장"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### 참고\n",
    "* [2] Using wrappers for Gensim models for working with Keras - https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/keras_wrapper.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Data 준비 (영어 - 20newsgroups)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### 필요한 라이브러리들"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import keras\n",
    "import numpy as np\n",
    "\n",
    "from gensim.models import word2vec\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.layers import Input, Dense, Flatten\n",
    "from keras.layers import Conv1D, MaxPooling1D\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### 데이터 준비 결과는 아래 세 리스트에 담긴다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "texts = []  # list of text samples\n",
    "texts_w2v = []  # used to train the word embeddings\n",
    "labels = []  # list of label ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### 20뉴스그룹 데이터의 일부분(카테고리 3가지 정도만)을 가져온다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#using 3 categories for training the classifier\n",
    "data = fetch_20newsgroups(subset='train', categories=['alt.atheism', 'comp.graphics', 'sci.space'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "dir(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "data.description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "len(data.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "len(data.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "data.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "data.target_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### 변환 과정을 한줄한줄 따져보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "for index in range(len(data)):\n",
    "    print(\"index:\", index)\n",
    "    label_id = data.target[index]\n",
    "    print(\"label_id:\",label_id)\n",
    "    #file_data = data.data[index]\n",
    "    #print(file_data)\n",
    "    #i = file_data.find('\\n\\n')  # skip header\n",
    "    #if i > 0:\n",
    "    #    file_data = file_data[i:]\n",
    "    #try:\n",
    "    #    curr_str = str(file_data)\n",
    "    #    sentence_list = curr_str.split('\\n')\n",
    "    #    for sentence in sentence_list:\n",
    "    #        sentence = (sentence.strip()).lower()\n",
    "    #        texts.append(sentence)\n",
    "    #        texts_w2v.append(sentence.split(' '))\n",
    "    #        labels.append(label_id)\n",
    "    #except:\n",
    "    #    None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "for index in range(len(data)):\n",
    "    print(\"index:\", index)\n",
    "    label_id = data.target[index]\n",
    "    print(\"label_id:\",label_id)\n",
    "    file_data = data.data[index]\n",
    "    print(\"file_data:\\n\",file_data)\n",
    "    #i = file_data.find('\\n\\n')  # skip header\n",
    "    #if i > 0:\n",
    "    #    file_data = file_data[i:]\n",
    "    #try:\n",
    "    #    curr_str = str(file_data)\n",
    "    #    sentence_list = curr_str.split('\\n')\n",
    "    #    for sentence in sentence_list:\n",
    "    #        sentence = (sentence.strip()).lower()\n",
    "    #        texts.append(sentence)\n",
    "    #        texts_w2v.append(sentence.split(' '))\n",
    "    #        labels.append(label_id)\n",
    "    #except:\n",
    "    #    None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### 첫번째 데이터만 가져와서 처리 과정을 차근차근 살펴보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "for index in range(1) : #len(data)):\n",
    "    print(\"index:\", index)\n",
    "    label_id = data.target[index]\n",
    "    print(\"label_id:\",label_id)\n",
    "    file_data = data.data[index]\n",
    "    print(\"file_data:\\n\",file_data)\n",
    "    #i = file_data.find('\\n\\n')  # skip header\n",
    "    #if i > 0:\n",
    "    #    file_data = file_data[i:]\n",
    "    #try:\n",
    "    #    curr_str = str(file_data)\n",
    "    #    sentence_list = curr_str.split('\\n')\n",
    "    #    for sentence in sentence_list:\n",
    "    #        sentence = (sentence.strip()).lower()\n",
    "    #        texts.append(sentence)\n",
    "    #        texts_w2v.append(sentence.split(' '))\n",
    "    #        labels.append(label_id)\n",
    "    #except:\n",
    "    #    None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "for index in range(1) : #len(data)):\n",
    "    print(\"index:\", index)\n",
    "    label_id = data.target[index]\n",
    "    print(\"label_id:\",label_id)\n",
    "    file_data = data.data[index]\n",
    "    print(\"file_data:\\n\",file_data)\n",
    "    i = file_data.find('\\n\\n')  # skip header\n",
    "    print(\"i:\", i)\n",
    "    #if i > 0:\n",
    "    #    file_data = file_data[i:]\n",
    "    #try:\n",
    "    #    curr_str = str(file_data)\n",
    "    #    sentence_list = curr_str.split('\\n')\n",
    "    #    for sentence in sentence_list:\n",
    "    #        sentence = (sentence.strip()).lower()\n",
    "    #        texts.append(sentence)\n",
    "    #        texts_w2v.append(sentence.split(' '))\n",
    "    #        labels.append(label_id)\n",
    "    #except:\n",
    "    #    None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "for index in range(1) : #len(data)):\n",
    "    print(\"index:\", index)\n",
    "    label_id = data.target[index]\n",
    "    print(\"label_id:\",label_id)\n",
    "    file_data = data.data[index]\n",
    "    print(\"file_data:\\n\",file_data)\n",
    "    i = file_data.find('\\n\\n')  # skip header\n",
    "    print(\"i:\", i)\n",
    "    if i > 0:\n",
    "        file_data = file_data[i:]\n",
    "        print(\"file_data(skip_heaer):\", file_data)\n",
    "    #try:\n",
    "    #    curr_str = str(file_data)\n",
    "    #    sentence_list = curr_str.split('\\n')\n",
    "    #    for sentence in sentence_list:\n",
    "    #        sentence = (sentence.strip()).lower()\n",
    "    #        texts.append(sentence)\n",
    "    #        texts_w2v.append(sentence.split(' '))\n",
    "    #        labels.append(label_id)\n",
    "    #except:\n",
    "    #    None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "for index in range(1) : #len(data)):\n",
    "    #print(\"index:\", index)\n",
    "    label_id = data.target[index]\n",
    "    #print(\"label_id:\",label_id)\n",
    "    file_data = data.data[index]\n",
    "    #print(\"file_data:\\n\",file_data)\n",
    "    i = file_data.find('\\n\\n')  # skip header\n",
    "    #print(\"i:\", i)\n",
    "    if i > 0:\n",
    "        file_data = file_data[i:]\n",
    "        #print(\"file_data(skip_heaer):\", file_data)\n",
    "    try:\n",
    "        curr_str = str(file_data)\n",
    "        print(\"curr_str\", curr_str)\n",
    "    #    sentence_list = curr_str.split('\\n')\n",
    "    #    for sentence in sentence_list:\n",
    "    #        sentence = (sentence.strip()).lower()\n",
    "    #        texts.append(sentence)\n",
    "    #        texts_w2v.append(sentence.split(' '))\n",
    "    #        labels.append(label_id)\n",
    "    except:\n",
    "        None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "for index in range(1) : #len(data)):\n",
    "    #print(\"index:\", index)\n",
    "    label_id = data.target[index]\n",
    "    #print(\"label_id:\",label_id)\n",
    "    file_data = data.data[index]\n",
    "    #print(\"file_data:\\n\",file_data)\n",
    "    i = file_data.find('\\n\\n')  # skip header\n",
    "    #print(\"i:\", i)\n",
    "    if i > 0:\n",
    "        file_data = file_data[i:]\n",
    "        #print(\"file_data(skip_heaer):\", file_data)\n",
    "    try:\n",
    "        curr_str = str(file_data)\n",
    "        print(\"curr_str\", curr_str)\n",
    "        sentence_list = curr_str.split('\\n')\n",
    "        print(\"sentence_list:\\n\", sentence_list)\n",
    "    #    for sentence in sentence_list:\n",
    "    #        sentence = (sentence.strip()).lower()\n",
    "    #        texts.append(sentence)\n",
    "    #        texts_w2v.append(sentence.split(' '))\n",
    "    #        labels.append(label_id)\n",
    "    except:\n",
    "        None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "for index in range(1) : #len(data)):\n",
    "    #print(\"index:\", index)\n",
    "    label_id = data.target[index]\n",
    "    #print(\"label_id:\",label_id)\n",
    "    file_data = data.data[index]\n",
    "    #print(\"file_data:\\n\",file_data)\n",
    "    i = file_data.find('\\n\\n')  # skip header\n",
    "    #print(\"i:\", i)\n",
    "    if i > 0:\n",
    "        file_data = file_data[i:]\n",
    "        #print(\"file_data(skip_heaer):\", file_data)\n",
    "    try:\n",
    "        curr_str = str(file_data)\n",
    "        print(\"curr_str\", curr_str)\n",
    "        sentence_list = curr_str.split('\\n')\n",
    "        print(\"sentence_list:\\n\", sentence_list)\n",
    "        for sentence in sentence_list:\n",
    "            sentence = (sentence.strip()).lower()\n",
    "            print(\"sentence:\\n\", sentence)\n",
    "    #        texts.append(sentence)\n",
    "    #        texts_w2v.append(sentence.split(' '))\n",
    "    #        labels.append(label_id)\n",
    "    except:\n",
    "        None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "texts = []  # list of text samples\n",
    "texts_w2v = []  # used to train the word embeddings\n",
    "labels = []  # list of label ids\n",
    "\n",
    "for index in range(1) : #len(data)):\n",
    "    #print(\"index:\", index)\n",
    "    label_id = data.target[index]\n",
    "    #print(\"label_id:\",label_id)\n",
    "    file_data = data.data[index]\n",
    "    #print(\"file_data:\\n\",file_data)\n",
    "    i = file_data.find('\\n\\n')  # skip header\n",
    "    #print(\"i:\", i)\n",
    "    if i > 0:\n",
    "        file_data = file_data[i:]\n",
    "        #print(\"file_data(skip_heaer):\", file_data)\n",
    "    try:\n",
    "        curr_str = str(file_data)\n",
    "        #print(\"curr_str\", curr_str)\n",
    "        sentence_list = curr_str.split('\\n')\n",
    "        #print(\"sentence_list:\\n\", sentence_list)\n",
    "        for sentence in sentence_list:\n",
    "            sentence = (sentence.strip()).lower()\n",
    "            print(\"sentence:\\n\", sentence)\n",
    "            texts.append(sentence)\n",
    "    #        texts_w2v.append(sentence.split(' '))\n",
    "    #        labels.append(label_id)\n",
    "    except:\n",
    "        None\n",
    "\n",
    "print(\"texts:\\n\", texts)\n",
    "print(\"texts_w2v:\\n\", texts_w2v)\n",
    "print(\"labels:\\n\", labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "texts = []  # list of text samples\n",
    "texts_w2v = []  # used to train the word embeddings\n",
    "labels = []  # list of label ids\n",
    "\n",
    "for index in range(1) : #len(data)):\n",
    "    #print(\"index:\", index)\n",
    "    label_id = data.target[index]\n",
    "    #print(\"label_id:\",label_id)\n",
    "    file_data = data.data[index]\n",
    "    #print(\"file_data:\\n\",file_data)\n",
    "    i = file_data.find('\\n\\n')  # skip header\n",
    "    #print(\"i:\", i)\n",
    "    if i > 0:\n",
    "        file_data = file_data[i:]\n",
    "        #print(\"file_data(skip_heaer):\", file_data)\n",
    "    try:\n",
    "        curr_str = str(file_data)\n",
    "        #print(\"curr_str\", curr_str)\n",
    "        sentence_list = curr_str.split('\\n')\n",
    "        #print(\"sentence_list:\\n\", sentence_list)\n",
    "        for sentence in sentence_list:\n",
    "            sentence = (sentence.strip()).lower()\n",
    "            #print(\"sentence:\\n\", sentence)\n",
    "            texts.append(sentence)\n",
    "            texts_w2v.append(sentence.split(' '))\n",
    "    #        labels.append(label_id)\n",
    "    except:\n",
    "        None\n",
    "\n",
    "print(\"texts:\\n\", texts)\n",
    "print(\"texts_w2v:\\n\", texts_w2v)\n",
    "print(\"labels:\\n\", labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "texts = []  # list of text samples\n",
    "texts_w2v = []  # used to train the word embeddings\n",
    "labels = []  # list of label ids\n",
    "\n",
    "for index in range(1) : #len(data)):\n",
    "    #print(\"index:\", index)\n",
    "    label_id = data.target[index]\n",
    "    #print(\"label_id:\",label_id)\n",
    "    file_data = data.data[index]\n",
    "    #print(\"file_data:\\n\",file_data)\n",
    "    i = file_data.find('\\n\\n')  # skip header\n",
    "    #print(\"i:\", i)\n",
    "    if i > 0:\n",
    "        file_data = file_data[i:]\n",
    "        #print(\"file_data(skip_heaer):\", file_data)\n",
    "    try:\n",
    "        curr_str = str(file_data)\n",
    "        #print(\"curr_str\", curr_str)\n",
    "        sentence_list = curr_str.split('\\n')\n",
    "        #print(\"sentence_list:\\n\", sentence_list)\n",
    "        for sentence in sentence_list:\n",
    "            sentence = (sentence.strip()).lower()\n",
    "            #print(\"sentence:\\n\", sentence)\n",
    "            texts.append(sentence)\n",
    "            texts_w2v.append(sentence.split(' '))\n",
    "            labels.append(label_id)\n",
    "    except:\n",
    "        None\n",
    "\n",
    "print(\"texts:\\n\", texts)\n",
    "print(\"texts_w2v:\\n\", texts_w2v)\n",
    "print(\"labels:\\n\", labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### 데이터 전체에 적용한 결과"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# full code \n",
    "\n",
    "texts = []  # list of text samples\n",
    "texts_w2v = []  # used to train the word embeddings\n",
    "labels = []  # list of label ids\n",
    "\n",
    "for index in range(len(data)):\n",
    "    label_id = data.target[index]\n",
    "    file_data = data.data[index]\n",
    "    i = file_data.find('\\n\\n')  # skip header\n",
    "    if i > 0:\n",
    "        file_data = file_data[i:]\n",
    "    try:\n",
    "        curr_str = str(file_data)\n",
    "        sentence_list = curr_str.split('\\n')\n",
    "        for sentence in sentence_list:\n",
    "            sentence = (sentence.strip()).lower()\n",
    "            texts.append(sentence)\n",
    "            texts_w2v.append(sentence.split(' '))\n",
    "            labels.append(label_id)\n",
    "    except:\n",
    "        None\n",
    "        \n",
    "        \n",
    "print(\"texts:\\n\", texts)\n",
    "print(\"texts_w2v:\\n\", texts_w2v)\n",
    "print(\"labels:\\n\", labels)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 데이터 준비 (한국어/실습)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "##### 다음 문서들을 이용해서 데이터를 만들자. \n",
    "* https://gasazip.com/view.html?no=614736\n",
    "* https://gasazip.com/1224697\n",
    "* https://gasazip.com/view.html?no=599082\n",
    "* https://gasazip.com/view.html?no=645465\n",
    "* http://gasazip.com/view.html?no=643505\n",
    "* https://gasazip.com/view.html?no=615362    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# -- coding \n",
    "\n",
    "texts_kr = []  # list of text samples\n",
    "texts_w2v_kr = []  # used to train the word embeddings\n",
    "labels_kr = []  # list of label ids\n",
    "\n",
    "# -- 여기에 데이터 준비하는 코드를 만들면 된다.        \n",
    "        \n",
    "print(\"texts:\\n\", texts_kr)\n",
    "print(\"texts_w2v:\\n\", texts_w2v_kr)\n",
    "print(\"labels:\\n\", labels_kr)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 딥러닝 모델에 입력가능한 포맷으로 변환  (영어 - 20newsgroups)\n",
    "we format our text samples and labels into tensors that can be fed into a neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# we rely on Keras utilities keras.preprocessing.text.Tokenizer and keras.preprocessing.sequence.pad_sequences.\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 1000\n",
    "\n",
    "# Vectorize the text samples into a 2D integer tensor\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "# word_index = tokenizer.word_index\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "labels = to_categorical(np.asarray(labels))\n",
    "\n",
    "x_train = data\n",
    "y_train = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "len(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sequences[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "labels[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 딥러닝 모델에 입력가능한 포맷으로 변환 (한국어/실습)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# -- coding\n",
    "\n",
    "#x_train_kr \n",
    "#y_train_kr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Keras에서 사용할 embedding layer 생성 (영어 - 20newsgroups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "Keras_w2v = word2vec.Word2Vec(min_count=1)\n",
    "Keras_w2v.build_vocab(texts_w2v)\n",
    "Keras_w2v.train(texts, total_examples=Keras_w2v.corpus_count, epochs=Keras_w2v.iter)\n",
    "Keras_w2v_wv = Keras_w2v.wv\n",
    "embedding_layer = Keras_w2v_wv.get_keras_embedding()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Keras에서 사용할 embedding layer 생성 (한국어/실습)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# -- coding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## CNN 모델 생성 및 학습 (영어 - 20newsgroups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Finally, we create a small 1D convnet to solve our classification problem.\n",
    "\n",
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "x = Conv1D(128, 5, activation='relu')(embedded_sequences)\n",
    "x = MaxPooling1D(5)(x)\n",
    "x = Conv1D(128, 5, activation='relu')(x)\n",
    "x = MaxPooling1D(5)(x)\n",
    "x = Conv1D(128, 5, activation='relu')(x)\n",
    "x = MaxPooling1D(35)(x)  # global max pooling\n",
    "x = Flatten()(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "preds = Dense(y_train.shape[1], activation='softmax')(x)\n",
    "\n",
    "model = Model(sequence_input, preds)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['acc'])\n",
    "\n",
    "model.fit(x_train, y_train, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "SVG(model_to_dot(model, show_shapes=True).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### CNN 모델 생성 및 학습 (한국어/실습)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# -- coding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 학습된 모델로 새로운 문장(or 문서) 분류하기 (영어 - 20newsgroups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def process_text(text):\n",
    "    \"\"\" \n",
    "    Process the input text by tokenizing and padding it.\n",
    "    \"\"\"\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(text)\n",
    "    x = tokenizer.texts_to_sequences(text)\n",
    "\n",
    "    x = pad_sequences(x, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "input_text = 'artificial intelligence'\n",
    "\n",
    "matrix = process_text(input_text)\n",
    "\n",
    "predictions = model.predict(matrix)\n",
    "\n",
    "categories=['alt.atheism', 'comp.graphics', 'sci.space']\n",
    "# get the actual categories from output\n",
    "scoredict = {}\n",
    "for idx, classlabel in zip(range(len(categories)), categories):\n",
    "    scoredict[classlabel] = predictions[0][idx]\n",
    "\n",
    "print(scoredict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 학습된 모델로 새로운 문장(or 문서) 분류하기 (한국어/실습)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# -- coding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "---------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 2D CNN for Text (Gensim + Keras)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### 참고\n",
    "* [3] https://github.com/bhaveshoswal/CNN-text-classification-keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense, Embedding, Conv2D, MaxPool2D\n",
    "from keras.layers import Reshape, Flatten, Dropout, Concatenate\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sequence_length = MAX_SEQUENCE_LENGTH\n",
    "vocabulary_size = len(Keras_w2v_wv.vocab.items())\n",
    "embedding_dim = 256\n",
    "filter_sizes = [3,4,5]\n",
    "num_filters = 512\n",
    "drop = 0.5\n",
    "\n",
    "epochs = 100\n",
    "batch_size = 30\n",
    "\n",
    "# this returns a tensor\n",
    "print(\"Creating Model...\")\n",
    "inputs = Input(shape=(sequence_length,), dtype='int32')\n",
    "embedding = Embedding(input_dim=vocabulary_size, output_dim=embedding_dim, input_length=sequence_length)(inputs)\n",
    "reshape = Reshape((sequence_length,embedding_dim,1))(embedding)\n",
    "\n",
    "conv_0 = Conv2D(num_filters, kernel_size=(filter_sizes[0], embedding_dim), padding='valid', kernel_initializer='normal', activation='relu')(reshape)\n",
    "conv_1 = Conv2D(num_filters, kernel_size=(filter_sizes[1], embedding_dim), padding='valid', kernel_initializer='normal', activation='relu')(reshape)\n",
    "conv_2 = Conv2D(num_filters, kernel_size=(filter_sizes[2], embedding_dim), padding='valid', kernel_initializer='normal', activation='relu')(reshape)\n",
    "\n",
    "maxpool_0 = MaxPool2D(pool_size=(sequence_length - filter_sizes[0] + 1, 1), strides=(1,1), padding='valid')(conv_0)\n",
    "maxpool_1 = MaxPool2D(pool_size=(sequence_length - filter_sizes[1] + 1, 1), strides=(1,1), padding='valid')(conv_1)\n",
    "maxpool_2 = MaxPool2D(pool_size=(sequence_length - filter_sizes[2] + 1, 1), strides=(1,1), padding='valid')(conv_2)\n",
    "\n",
    "concatenated_tensor = Concatenate(axis=1)([maxpool_0, maxpool_1, maxpool_2])\n",
    "flatten = Flatten()(concatenated_tensor)\n",
    "dropout = Dropout(drop)(flatten)\n",
    "output = Dense(units=3, activation='softmax')(dropout)\n",
    "\n",
    "# this creates a model that includes\n",
    "model = Model(inputs=inputs, outputs=output)\n",
    "\n",
    "checkpoint = ModelCheckpoint('weights.{epoch:03d}-{val_acc:.4f}.hdf5', monitor='val_acc', verbose=1, save_best_only=True, mode='auto')\n",
    "adam = Adam(lr=1e-4, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "\n",
    "model.compile(optimizer=adam, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "print(\"Traning Model...\")\n",
    "model.fit(x_train, y_train, epochs=5) # batch_size=batch_size, epochs=epochs, verbose=1, callbacks=[checkpoint], validation_data=(X_test, y_test))  # starts training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "SVG(model_to_dot(model, show_shapes=True).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "input_text = 'artificial intelligence'\n",
    "\n",
    "matrix = process_text(input_text)\n",
    "\n",
    "predictions = model.predict(matrix)\n",
    "\n",
    "categories=['alt.atheism', 'comp.graphics', 'sci.space']\n",
    "# get the actual categories from output\n",
    "scoredict = {}\n",
    "for idx, classlabel in zip(range(len(categories)), categories):\n",
    "    scoredict[classlabel] = predictions[0][idx]\n",
    "\n",
    "print(scoredict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 참고자료\n",
    "* [2] Using wrappers for Gensim models for working with Keras - https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/keras_wrapper.ipynb\n",
    "* [3] https://github.com/bhaveshoswal/CNN-text-classification-keras\n",
    "* [4] https://github.com/keras-team/keras/blob/ce406b773b9f36be5718a4369ad07fea4f9ebdba/examples/imdb_cnn.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "name": "Gensim Newsgroup.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
