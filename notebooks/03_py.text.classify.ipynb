{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03. 텍스트 클래스 분류 맛보기 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 싸이그래머 / 어바웃 파이썬\n",
    "* 곽대기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 트레이닝 데이터 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import bs4\n",
    "\n",
    "inputurl = 'http://edition.cnn.com/2018/01/19/europe/putin-icy-dip-intl/index.html'\n",
    "htmlData = urllib.request.urlopen(inputurl)\n",
    "bs = bs4.BeautifulSoup(htmlData, 'lxml')\n",
    "bodies = bs.findAll('h1', 'pg-headline')\n",
    "bodies += bs.findAll('p', 'zn-body__paragraph')\n",
    "bodies += bs.findAll('div', 'zn-body__paragraph')\n",
    "inputstr = \"\"\n",
    "for body in bodies:\n",
    "    inputstr += (body.getText() + \" \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Vladimir Putin takes dip in freezing water to mark Epiphany  (CNN)Russian President Vladimir Putin stripped down to his bathing trunks in freezing temperatures Friday before immersing himself in icy waters to mark the feast of the Epiphany.  Putin bathed in Lake Seliger, around 250 miles north of Moscow, as part of a traditional Orthodox Christian ritual which commemorates the baptism of Jesus. After removing his fur coat and boots, the 65-year-old entered the pool, crossed himself and momentarily placed his head under the freezing water. The annual ceremony took place after Putin had completed a trip to St. Petersburg and the Leningrad Region, followed by a visit to the St. Nilus Stolobensky Monastery in the Tver Region.  Putin's icy dip isn't the first time he has stripped down in public. Last August he was photographed topless while fishing. In 2009, a photo was taken of him bare-chested while riding a horse on vacation. \""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputstr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 모델 준비"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "코사인 유사도\n",
    "\n",
    "코사인 유사도(― 類似度, 영어: cosine similarity)는 내적공간의 두 벡터간 각도의 코사인값을 이용하여 측정된 벡터간의 유사한 정도를 의미한다. 각도가 0°일 때의 코사인값은 1이며, 다른 모든 각도의 코사인값은 1보다 작다. 따라서 이 값은 벡터의 크기가 아닌 방향의 유사도를 판단하는 목적으로 사용되며, 두 벡터의 방향이 완전히 같을 경우 1, 90°의 각을 이룰 경우 0, 180°로 완전히 반대 방향인 경우 -1의 값을 갖는다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "![cosineSIM.PNG](attachment:cosineSIM.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "def cosSimilarity(A, B):\n",
    "    multi = (A.dot(B))\n",
    "    x = math.sqrt(A.dot(A))\n",
    "    y = math.sqrt(B.dot(B))\n",
    "    result = multi / (x * y)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "단어목록 만들기\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Set of vocabularies with indices\n",
    "class Vocabulary:\n",
    "    def __init__(self):\n",
    "        self.vector = {}\n",
    "    def add(self, tokens):\n",
    "        for token in tokens:\n",
    "            if token not in self.vector and not token.isspace() and token != '':\n",
    "                self.vector[token] = len(self.vector)\n",
    "    def indexOf(self, vocab):\n",
    "        return self.vector[vocab]\n",
    "    def size(self):\n",
    "        return len(self.vector)\n",
    "    def at(self, i): # get ith word in the vector\n",
    "        return list(self.vector)[i]\n",
    "    # vectorize = dict -> numpy.array\n",
    "    def vectorize(self, word):\n",
    "        v = [0 for i in range(self.size())]\n",
    "        if word in self.vector:\n",
    "            v[self.indexOf(word)] = 1\n",
    "        else:\n",
    "            print(\"<ERROR> Word \\'\" + word + \"\\' Not Found\")\n",
    "        return np.array(v)\n",
    "    def save(self, filename):\n",
    "        f = open(filename, 'w', encoding='utf-8')\n",
    "        for word in self.vector:\n",
    "            f.write(word + '\\n')\n",
    "        f.close()\n",
    "    def load(self, filename):\n",
    "        f = open(filename, 'r', encoding='utf-8')\n",
    "        lines = f.readlines()\n",
    "        bow = [i[:-1] for i in lines]\n",
    "        self.add(bow)\n",
    "        f.close()\n",
    "    def __str__(self):\n",
    "        s = \"Vocabulary(\"\n",
    "        for word in self.vector:\n",
    "            s += (str(self.vector[word]) + \": \" + word + \", \")\n",
    "        if self.size() != 0:\n",
    "            s = s[:-2]\n",
    "        s += \")\"\n",
    "        return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 분류 모델 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "어휘 뭉치 생성 전에 전처리 작업이 필요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# preprocess = str -> nltk.Text\n",
    "def preprocess(inputstr):\n",
    "    inputstr = inputstr.lower()\n",
    "    tokens = nltk.word_tokenize(inputstr)\n",
    "    stpwrds = set(stopwords.words('english'))\n",
    "    tokens = [i for i in tokens if i not in stpwrds and i.isalpha()]\n",
    "    stemmer = nltk.stem.porter.PorterStemmer()\n",
    "    stems = [stemmer.stem(i) for i in tokens]\n",
    "    text = nltk.Text(stems)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: ''",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-a511427028f9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mgolf_training_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmyvoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVocabulary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPATH_TRAINING_DATA\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mgolf_training_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: ''"
     ]
    }
   ],
   "source": [
    "PATH_TRAINING_DATA = ''\n",
    "golf_training_file = ''\n",
    "myvoc = Vocabulary()\n",
    "f = open(PATH_TRAINING_DATA + golf_training_file)\n",
    "lines = f.readlines()\n",
    "for line in lines:\n",
    "    tmp = getTextFromURL(line)\n",
    "    bow = preprocess(tmp)\n",
    "    myvoc.add(bow)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<ERROR> Word 'apple' Not Found\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([], dtype=float64)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myvoc.vectorize(\"apple\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "![vectorSUM.PNG](attachment:vectorSUM.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "이 개념을 통해 트레이닝 데이터셋의 모든 문서들을 각각 벡터로 만들고, 주제별로 평균을 내도록 하자. 이 평균값이 해당 주제에 대한 대표 벡터이자 분류 기준이라고 보면 된다. 참고로 이 글에서는 Cosine Similarity를 사용하기 때문에, 평균을 구하지 않고 각 벡터들의 합만 구해도 된다. 벡터의 방향성이 얼마나 유사한지를 판단하는 방법이라 벡터의 길이는 상관이 없기 때문이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "![plot.PNG](attachment:plot.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 향후 발전 방향"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "이 기초적인 프로젝트를 개선하고 확장하면 더욱 높은 성능을 기대할 수 있다. 이를 위해서 어떠한 점들을 개선해야 하는지 몇 가지 살펴보도록 하겠다.\n",
    "\n",
    "우선, 각 벡터 사이의 유사도를 측정하는데 있어서 Cosine Similarity 이외에 자카드 유사도(Jaccard Similarity)를 구하는 방법도 많이 쓰인다. 두 방법 중 한 가지를 선택하거나, 둘을 적절히 조합하여 유사도를 측정하는 방식을 쓸 수도 있다. 모델을 설계하는 단계에서 입맛대로 사용하면 된다.\n",
    "\n",
    "또한 Vocabulary를 생성하는 과정에서 Stop Word들을 제거하거나 Stemmer를 사용함에 있어서 과연 무조건적인 Stop Words의 배제가 바람직한 것인지, 특정 Stemmer의 성능이 어떠한 경우에 좋은지 등 추가적으로 고려할 요소들이 많다.\n",
    "\n",
    "사실, 가장 중요한 부분은 바로 문서, 단어를 벡터화하는 방식이다. 이 글에서는 그저 단어들의 빈도수를 도출해서 그것들을 합한 것에 지나지 않지만, 실제 정보 검색 분야나 자연어처리 분야에서는 Doc2Vec, Word2Vec 등 머신러닝 기반의 다양한 벡터화 방법론이나 TF-IDF처럼 문서와 관련하여 각 단어의 중요도에 대해 고려하는 통계적 기법들이 널리 활용되고 있다. 특히 TF-IDF는 각 문서의 주제와 직결되는 중요한 단어들이 무엇인지 고려해준다는 점에서, 앞서 언급했던 Stop Word의 존재에 대한 걱정거리를 어느정도 덜어주기도 한다.\n",
    "\n",
    "이러한 다양한 방법들을 적절히 조합하고 적용하면 더욱더 강력한 텍스트 분류 모델이 완성될 것이다. 이 글의 토이 프로젝트를 통해 가장 기본적인 요소들에 대해 학습하고, 향후 개선과 확장을 통해 고도의 분류 시스템을 설계하는 단계까지 나아가도록 하자."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
